{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90edd15a00cb190c2a6667d7a3385aaf8c402bdb"
   },
   "source": [
    "# Trabalho Final da Disciplina de Deep Learning 1º/2018\n",
    "Segundo Trabalho da disciplina. Profº Rodrigo da Silva Guerra\n",
    "\n",
    "Objetivo: Implementar uma rede neural (tipo LSTM) utilizando um modelo treinado de palavras com análise por sintaxe (word2vec). \n",
    "\n",
    "Link 1: [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "Link 2: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "## Parte 1: Word2Vec\n",
    "A primeira parte do trabalho consiste em treinar um codificador Word2Vec seguindo uma das seguintes propostas:\n",
    "\n",
    "* Opção 1: Treinar Word2Vec utilizando vocabulário na língua portuguesa, utilizando ferramenta padrão com implementação pronta, tal como gensim.\n",
    "* Opção 2: Treinar Word2Vec utilizando vocabulário na língua inglesa, mas nesse caso implementando manualmente o algoritmo de treinamento utilizando uma biblioteca tal como TensorFlow.\n",
    "\n",
    "Em ambos os casos os autores devem demonstrar resultados satisfatórios do treinamento, mostrando como o embedding criado agrupa palavras de categoria semanticamente afim, e mostrando resultados coerentes em operações do tipo rei - homem + mulher = rainha.\n",
    "\n",
    "Para esta etapa será utilizado a **opção 1** com um dataset de notícias obtido pelo site Kaggle, da Folha de São Paulo. [Link 3](https://www.kaggle.com/marlesson/news-of-the-site-folhauol)\n",
    "\n",
    "## Parte 2: LSTM\n",
    "A segunda parte do trabalho consiste em implementar uma LSTM seguindo uma das seguintes propostas:\n",
    "* Opção 1: Treinar LSTM para classificar sentimentos (ou classe de documento, ou outro tipo de classificação similar). Neste caso a rede deve ser treinada utilizando embeddings que se aproveitam do codificador Word2Vec implementado na Parte 1.\n",
    "* Opção 2: Treinar LSTM para uma tarefa de predição da próxima palavra. Neste caso se treina a LSTM usando como entrada uma sequência de palavras de um texto, onde a rede neural deve prever a próxima palavra. Também neste caso a rede neural deve ser treinada utilizando os embeddings criados na Parte 1.\n",
    "* Opção 3: Treinar LSTM para outra tarefa de predição, não necessariamente ligada a processamento de linguagem natural. Essa opção deve ter uma proposta elaborada em detalhe e aprovada pelo professor. Apenas propostas aprovadas serão consideradas para avaliação.\n",
    "\n",
    "Para esta etapa será utilizado a **opção 1** com um dataset de frases do site *pensador.com* obtido e classicado pelo colega José Augusto Thomas ([Kaggle Profile](https://www.kaggle.com/gutothomas)) no site Kaggle, com frases de felicidade, ódio e tristeza. [Link 4](https://www.kaggle.com/gutothomas/phrases-about-happiness-from-pensadorcom), [Link 5](https://www.kaggle.com/gutothomas/phrases-about-hate-from-pensadorcom) e [Link 6](https://www.kaggle.com/gutothomas/phrases-about-sadness-from-pensadorcom).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "91364adc76213fee59a840ab91f82b05c114f7d1"
   },
   "source": [
    "## Importações e localização dos datasets\n",
    "Instalação do módulo gensim que possui o modelo para treino do word2vec e importação de bibliotecas para manipulação de dados e de estruturação e treinamento da rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a4dc0271fd7084c9dbe14a2730a0b12ea2a8cc1"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import string\n",
    "import json\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/news-of-the-site-folhauol/\"))\n",
    "print(os.listdir(\"../input/phrases-about-happiness-from-pensadorcom/\"))\n",
    "print(os.listdir(\"../input/phrases-about-hate-from-pensadorcom/\"))\n",
    "print(os.listdir(\"../input/phrases-about-sadness-from-pensadorcom/\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7edadede3f8f399d23fa1330c5b9b202b1c6016c"
   },
   "source": [
    "## Descrição de Funções\n",
    "Algumas funções para manipulação do texto obtido pelo dataset de notícias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e655aa1219e0b0ba82bfb5a698e43e0cd3adb2eb"
   },
   "outputs": [],
   "source": [
    "# # functions\n",
    "\n",
    "def remove_punctuation(phrase):\n",
    "    phrase_aux = phrase\n",
    "    for punc in string.punctuation:\n",
    "        phrase_aux = phrase_aux.replace(punc, ' ')\n",
    "    return phrase_aux\n",
    "\n",
    "\n",
    "def remove_punctuation_and_split_sentence(phrase):\n",
    "    phrase_aux = phrase\n",
    "    phrase_aux = remove_punctuation(phrase_aux)\n",
    "    phrase_aux = remove_punctuation(phrase_aux).split()\n",
    "    return phrase_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6fa9ef7b1620c548a6ece8bb3c53277304bbe9ba"
   },
   "source": [
    "## Preparação do dataset de notícias\n",
    "\n",
    "O dataset é importado e logo após é separados somente o necessário para o treinamento do word2vec, ver mais informações do dataset no [Link 3](https://www.kaggle.com/marlesson/news-of-the-site-folhauol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../input/news-of-the-site-folhauol/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f92f50d0d5eaf8ff193ea87b659370c18b7cdc3c"
   },
   "outputs": [],
   "source": [
    "textos = articles['title'].tolist() + articles['text'].tolist()\n",
    "del articles\n",
    "np.random.shuffle(textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fee5943ba4b5b738aa62ffd0c56808435fb0ced3"
   },
   "source": [
    "Cada texto do dataset é processado para ser tranformado em uma frase. Após a frase é livre de pontuações e tranformada numa lista de palavras pela função **remove_punctuation_and_split_sentence($\\cdot$)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f63b6beb26c60b55362aea32c411fc71bfcaa149"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for texto in textos:\n",
    "    texto = str(texto)\n",
    "    texto_splited = texto.lower().split('.')\n",
    "    sentences.extend(texto_splited)\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentences_list.append(remove_punctuation_and_split_sentence(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d9f0f4da293772e57ffe300a3bb238812167f75"
   },
   "source": [
    "## Treino do modelo com as notícias e teste do modelo\n",
    "\n",
    "Após a formatação correta de todo o dataset este pode ser utilizado pelos métodos da biblioteca gensim para a criação do modelo word2vec. Os métodos *Phrases* e *Phraser* servem para pré-condicionarem os dados do dataset, unindo palavras que se repetem na sentença como \"nova\" e \"iorque\", \"baleia\" e \"jubarte\", \"sao\" e \"paulo\", por exemplo. A classe Word2Vec atribuida a **model** de fato constrói o modelo de word2vec para ser usado posteriormente pela LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b28eb476b57747839355eede82e10d6ddf7a79c"
   },
   "outputs": [],
   "source": [
    "size_word2vec = 100\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "phrases = Phrases(sentences_list, min_count=5)\n",
    "bigram = Phraser(phrases)\n",
    "model = Word2Vec(bigram[sentences_list], size=size_word2vec, workers=8, compute_loss=True)\n",
    "# model = Word2Vec(sentences_list, size=size_word2vec, workers=8, compute_loss=True)\n",
    "training_loss = model.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72e3823c96b75ce723cce13540c9b52ae5d130c9"
   },
   "source": [
    "## Testes do modelo\n",
    "Abaixo um teste fazendo operações com os vetores do modelo e mais adiante uma análise utilizando TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44f5c82038eae05b5ba7a8c21d9b988ad8ad20db"
   },
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['mulher', 'rei'], negative=['homem']))\n",
    "print('\\n\\n')\n",
    "print(model.wv.most_similar('mulher'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51dcd50bcdf65654b62c6e189d26e2fd17e521df"
   },
   "source": [
    "### TSNE\n",
    "\n",
    "O TSNE é uma forma de reduzir a dimensão dos dados para melhor avaliá-los graficamente. Mais informações no [Link 7](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "\n",
    "Foi feita a escolha de $num\\_points$ palavras aleatoriamente de todo o vocabulário do modelo, redimensionado pelo modelo TSNE e plotado no gráfico abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f626f0fed66e931564b764e8349586cda5277db3"
   },
   "outputs": [],
   "source": [
    "num_points = 300\n",
    "\n",
    "ind = np.array(list(range(len(model.wv.vocab))))\n",
    "np.random.shuffle(ind)\n",
    "ind = ind[:num_points]\n",
    "\n",
    "labels     = [model.wv.index2word[i] for i in ind]\n",
    "embeddings = [model.wv.get_vector(word) for word in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5f2490e613c24ce6347615457ab915ab952cafc"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=50, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42c0d911c77422bf77f5207ec834c3bb4104b759"
   },
   "outputs": [],
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "plot(two_d_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3316c4b586f3c8b6ff27022222e069e2c0426227"
   },
   "source": [
    "## Preparação do dataset de frases\n",
    "\n",
    "O dataset já estava pré-condicionado e semi-padronizado (case sensitive) utiliza o formato .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "942671aefbfd3a57b495be7f4fe031078670254f"
   },
   "outputs": [],
   "source": [
    "with open('../input/phrases-about-happiness-from-pensadorcom/frases_sobre_felicidade.json') as f:\n",
    "    happiness_data = json.load(f)\n",
    "with open(\"../input/phrases-about-hate-from-pensadorcom/frases_de_odio.json\") as f:\n",
    "    hate_data = json.load(f)\n",
    "with open(\"../input/phrases-about-sadness-from-pensadorcom/frases-tristes.json\") as f:\n",
    "    sadness_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23e86ad830da3ef857a4e68740784d4a29a08d8e"
   },
   "source": [
    "Concatena-se os três datasets e se atribui uma *label* **target** para cada bloco de dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0beb0d9165cb0948128e940cfdb3c4180976a32f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mergeDataSetAndSetTarget(data1_LSTM, data2_LSTM, data3_LSTM):\n",
    "\n",
    "    data_LSTM = data1_LSTM + data2_LSTM + data3_LSTM\n",
    "\n",
    "    target_LSTM = np.zeros((len(data_LSTM), 3))\n",
    "    \n",
    "    # Targets:\n",
    "    # [1,0,0]: frases sobre felicidade\n",
    "    # [0,1,0]: frases sobre ódio\n",
    "    # [0,0,1]: frases sobre tristeza\n",
    "    \n",
    "    target_LSTM[:len(data1_LSTM)] = np.array([1,0,0])\n",
    "    target_LSTM[len(data1_LSTM):len(data1_LSTM)+len(data2_LSTM)] = np.array([0,1,0])\n",
    "    target_LSTM[len(data1_LSTM)+len(data2_LSTM):] = np.array([0,0,1])\n",
    "    \n",
    "    return data_LSTM, target_LSTM\n",
    "\n",
    "data_LSTM, target_LSTM = mergeDataSetAndSetTarget(happiness_data, hate_data, sadness_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a63eb46125956508220ab174fe5a27d7f324d341"
   },
   "source": [
    "O treinamento será feito por **Janela deslizante** logo as frases devem ter no mínimo o tamanho da janela $window\\_size$ . A rotina abaixo verfica se as frases são maiores ou iguais, senão um valor *unknown* **UNK** é inserido na lista de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3d0c2f546a87e4cb07756d64c9d7425e81225dd"
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "data_LSTM_pad = []\n",
    "\n",
    "for data in data_LSTM:\n",
    "    size = len(data)\n",
    "    if size < window_size:\n",
    "        pad_size = window_size - size\n",
    "        data_LSTM_pad.append(data + ['UNK']*pad_size)\n",
    "    else:\n",
    "        data_LSTM_pad.append(data)\n",
    "\n",
    "data_LSTM = data_LSTM_pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08d48c48363e2e5604607ba12c85338b618667ad"
   },
   "source": [
    "Criação dos pacotes para treinamento (batches) para o treinamento na LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6873633a47024076f455281ee78cd34544951233"
   },
   "outputs": [],
   "source": [
    "data_batches = []\n",
    "target_batches = []\n",
    "\n",
    "for data, target in zip(data_LSTM, target_LSTM):\n",
    "    size = len(data)\n",
    "    i = 0\n",
    "    while (i + window_size) <= size:\n",
    "        data_batches.append(data[i:i+window_size])\n",
    "        target_batches.append(target)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8a33b6860fc513ad2cc52169585a00ab4eefde0"
   },
   "source": [
    "## Remapeamento dos batches\n",
    "\n",
    "Transforma as strings dos batches no vetor correspondente no modelo word2vec treinado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da750bafa20fc6a3ce31068a601127d26a40b869"
   },
   "outputs": [],
   "source": [
    "sentences_list_w2v = []\n",
    "\n",
    "sentences_list = data_batches\n",
    "\n",
    "for sentence in sentences_list:\n",
    "    sentence_w2v = []\n",
    "#     print(sentence)\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            sentence_w2v.append(model.wv.get_vector(word))\n",
    "            #print(model.wv.get_vector(word))\n",
    "        except KeyError:\n",
    "            sentence_w2v.append(np.zeros(size_word2vec))\n",
    "    sentences_list_w2v.append(sentence_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd60e4b66bceb831670386881aa2c0a9d9ee4e87"
   },
   "source": [
    "Atribuição dos dados em batch para as palavras remapeadas pelo modelo **word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d843d0eb83df88cecca03d03cffada68fb83bb62"
   },
   "outputs": [],
   "source": [
    "data_batches = sentences_list_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "622314e64992983456301b788c98e1183c6918f0"
   },
   "source": [
    "Mistura dos batches  e separação para entre treino e testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ddffc8b1701a808e5b4b2396d2fef8960469fc8"
   },
   "outputs": [],
   "source": [
    "ind = np.array(range(len(data_batches)))\n",
    "\n",
    "data_batches_ar          = np.array(data_batches)\n",
    "target_batches_ar        = np.array(target_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50227ff0ce4869b21ca68530267167938ba2762e"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(ind)\n",
    "\n",
    "data_batches_sh   = data_batches_ar[ind]\n",
    "target_batches_sh = target_batches_ar[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60088171ff330570101c69f65297430039e246df"
   },
   "outputs": [],
   "source": [
    "size_train = int(len(data_batches_sh) * (9./10.))\n",
    "\n",
    "x_train = data_batches_sh[:size_train]\n",
    "y_train = target_batches_sh[:size_train]\n",
    "\n",
    "x_test = data_batches_sh[size_train:]\n",
    "y_test = target_batches_sh[size_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61041ba101131ac6a56b0ebe9817d153a774e24"
   },
   "source": [
    "## Construção da rede neural\n",
    "Descrição das camadas da rede neural e da configuração para o treinamento (**.compile($\\cdot$)**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e765c7df200c569d227ef7c092053a93a6ec571b"
   },
   "outputs": [],
   "source": [
    "modelRNN = Sequential()\n",
    "\n",
    "modelRNN.add(LSTM(64, input_shape=(5, 100)))\n",
    "modelRNN.add(Dense(3))\n",
    "modelRNN.add(Dropout(0.5))\n",
    "modelRNN.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c5234358c65467d1e2c675654091015b6904577"
   },
   "outputs": [],
   "source": [
    "modelRNN.compile(loss='mean_squared_error', \n",
    "              optimizer='adam', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b4d19a451c4f12bd9920137eaa676d2a6d63662"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65ac69b71e112b92c51f0ee698296da43230a822",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRNN.fit(x_train, y_train, epochs=200, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23396197ab61f6f23838d12a67227c1f94c36702"
   },
   "source": [
    "## Verificação do modelo\n",
    "\n",
    "Utiliza a parte de testes do dataset para verificar 1) o erro quadrático da rede neural e 2) o quanta a rede acerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44f7bd12abbfb23f90104cb96541cd28bc51169d"
   },
   "outputs": [],
   "source": [
    "modelRNN.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad17c7d6feb3d7bc62945b5240cfb38826778ba7"
   },
   "source": [
    "## Predições\n",
    "\n",
    "Utiliza a parte de testes para imprimir a saída da rede e imprime junto o valor alvo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a18cd25f9b7aed754458538baabf80ce4dbc595f"
   },
   "outputs": [],
   "source": [
    "print(modelRNN.predict(x_test))\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
